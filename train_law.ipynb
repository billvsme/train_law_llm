{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdJmJPb07U47"
      },
      "outputs": [],
      "source": [
        "\"\"\"‰∏ãËΩΩ\n",
        "LLaMA-Factory Áî®‰∫éÂæÆË∞É\n",
        "DISC-Law-SFT Ê≥ïÂæãÊï∞ÊçÆ\n",
        "\"\"\"\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory\n",
        "!git clone https://huggingface.co/datasets/ShengbinYue/DISC-Law-SFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o55DdL4NcS9E"
      },
      "outputs": [],
      "source": [
        "\"\"\"Âú®LLaMA-Factory‰∏≠Ê∑ªÂä†DISC-Law-SFT Ê≥ïÂæãÊï∞ÊçÆ\n",
        "\"\"\"\n",
        "!apt-get install -y jq\n",
        "!cp /content/DISC-Law-SFT/DISC-Law-SFT-Pair.jsonl /content/LLaMA-Factory/data/\n",
        "!cp /content/DISC-Law-SFT/DISC-Law-SFT-Triplet-released.jsonl /content/LLaMA-Factory/data/\n",
        "!jq '.law_sft_pair={\"file_name\": \"DISC-Law-SFT-Pair.jsonl\", \"columns\": {\"prompt\": \"input\", \"response\": \"output\"}}' /content/LLaMA-Factory/data/dataset_info.json > new_dataset_info.json\n",
        "!cp  new_dataset_info.json /content/LLaMA-Factory/data/dataset_info.json\n",
        "!jq '.law_sft_triplet={\"file_name\": \"DISC-Law-SFT-Triplet-released.jsonl\", \"columns\": {\"prompt\": \"input\", \"response\": \"output\"}}' /content/LLaMA-Factory/data/dataset_info.json > new_dataset_info.json\n",
        "!cp  new_dataset_info.json /content/LLaMA-Factory/data/dataset_info.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h43G1zhf7msU"
      },
      "outputs": [],
      "source": [
        "\"\"\"ÂÆâË£Ö‰æùËµñ\n",
        "\"\"\"\n",
        "%cd /content/LLaMA-Factory\n",
        "!pip install -r requirements.txt\n",
        "!pip install einops\n",
        "!pip install transformers==4.34.0\n",
        "!pip install deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFqfOfzQ-8Sd"
      },
      "outputs": [],
      "source": [
        "\"\"\"ÊõøÊç¢Ëá™ÊàëËÆ§Áü•self_cognitionÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÂêçÁß∞\n",
        "\"\"\"\n",
        "%cd /content/LLaMA-Factory\n",
        "!sed -i 's/<NAME>/Ê≥ïÂæãAI/g' data/self_cognition.json\n",
        "!sed -i 's/<AUTHOR>/billvsme/g' data/self_cognition.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"ÁîüÊàêdeepspeedÈÖçÁΩÆÊñá‰ª∂\n",
        "\"\"\"\n",
        "%cd /content/LLaMA-Factory\n",
        "!echo '''{\\\n",
        "  \"train_batch_size\": \"auto\",\\\n",
        "  \"train_micro_batch_size_per_gpu\": \"auto\",\\\n",
        "  \"gradient_accumulation_steps\": \"auto\",\\\n",
        "  \"gradient_clipping\": \"auto\",\\\n",
        "  \"zero_allow_untested_optimizer\": true,\\\n",
        "  \"fp16\": {\\\n",
        "    \"enabled\": \"auto\",\\\n",
        "    \"loss_scale\": 0,\\\n",
        "    \"initial_scale_power\": 16,\\\n",
        "    \"loss_scale_window\": 1000,\\\n",
        "    \"hysteresis\": 2,\\\n",
        "    \"min_loss_scale\": 1\\\n",
        "  },\\\n",
        "  \"zero_optimization\": {\\\n",
        "    \"stage\": 2,\\\n",
        "    \"offload_optimizer\": {\\\n",
        "      \"device\": \"cpu\",\\\n",
        "      \"pin_memory\": true\\\n",
        "    },\\\n",
        "    \"allgather_partitions\": true,\\\n",
        "    \"allgather_bucket_size\": 5e8,\\\n",
        "    \"reduce_scatter\": true,\\\n",
        "    \"reduce_bucket_size\": 5e8,\\\n",
        "    \"overlap_comm\": false,\\\n",
        "    \"contiguous_gradients\": true\\\n",
        "  }\\\n",
        "}''' > ds_config.json"
      ],
      "metadata": {
        "id": "qSf1qpsoW7cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZmSHoI_8Jo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38991347-6698-43f6-f0c8-147a6713a7e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "[2023-11-07 03:32:23,110] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-11-07 03:32:26,052] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2023-11-07 03:32:26,052] [INFO] [runner.py:570:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed ds_config.json --stage sft --model_name_or_path microsoft/phi-1_5 --do_train True --finetuning_type lora --template vanilla --flash_attn False --shift_attn False --dataset_dir data --dataset self_cognition,law_sft_triplet --cutoff_len 1664 --learning_rate 2e-04 --num_train_epochs 5.0 --max_samples 1000 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --max_grad_norm 1.0 --logging_steps 5 --save_steps 1000 --warmup_steps 0 --neft_alpha 0 --train_on_prompt False --upcast_layernorm False --lora_rank 8 --lora_dropout 0.1 --lora_target Wqkv --resume_lora_training True --output_dir saves/Phi1.5-1.3B/lora/law --fp16 True --plot_loss True\n",
            "[2023-11-07 03:32:28,399] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2023-11-07 03:32:30,589] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n",
            "[2023-11-07 03:32:30,589] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-11-07 03:32:30,589] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n",
            "[2023-11-07 03:32:30,589] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2023-11-07 03:32:30,589] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n",
            "[2023-11-07 03:32:30,589] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2023-11-07 03:32:30,589] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n",
            "[2023-11-07 03:32:30,589] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2023-11-07 03:32:30,589] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2023-11-07 03:32:30,589] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2023-11-07 03:32:30,589] [INFO] [launch.py:163:main] dist_world_size=1\n",
            "[2023-11-07 03:32:30,589] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "2023-11-07 03:32:34.874215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-07 03:32:34.874276: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-07 03:32:34.874312: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-07 03:32:36.522777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-11-07 03:32:38,185] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.\n",
            "[2023-11-07 03:32:40,757] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2023-11-07 03:32:40,757] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "11/07/2023 03:32:40 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1345] 2023-11-07 03:32:40,763 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-07 03:32:40,763 >> PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1711: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "11/07/2023 03:32:40 - INFO - llmtuner.tuner.core.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
            "  distributed training: True, compute dtype: torch.float16\n",
            "11/07/2023 03:32:40 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=ds_config.json,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saves/Phi1.5-1.3B/lora/law/runs/Nov07_03-32-40_9254c2e7c264,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=saves/Phi1.5-1.3B/lora/law,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=1,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saves/Phi1.5-1.3B/lora/law,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/07/2023 03:32:40 - INFO - llmtuner.dsets.loader - Loading dataset self_cognition.json...\n",
            "11/07/2023 03:32:40 - WARNING - llmtuner.dsets.utils - Checksum failed: mismatched SHA-1 hash value at data/self_cognition.json.\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=None' instead.\n",
            "  warnings.warn(\n",
            "Using custom data configuration default-553a64b70ec753a7\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-553a64b70ec753a7/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-553a64b70ec753a7/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 9532.51it/s]\n",
            "Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1550.57it/s]\n",
            "Generating train split\n",
            "Generating train split: 80 examples [00:00, 4061.74 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-553a64b70ec753a7/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "11/07/2023 03:32:41 - INFO - llmtuner.dsets.loader - Loading dataset DISC-Law-SFT-Triplet-released.jsonl...\n",
            "11/07/2023 03:32:41 - WARNING - llmtuner.dsets.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
            "Using custom data configuration default-85c606e94a1c2fb5\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-85c606e94a1c2fb5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-85c606e94a1c2fb5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-85c606e94a1c2fb5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Some of the datasets have disparate format. Resetting the format of the concatenated dataset.\n",
            "[INFO|tokenization_utils_base.py:2043] 2023-11-07 03:32:41,411 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/92557d03bb12543040c8bb5f0475cbdd9968f05f/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2043] 2023-11-07 03:32:41,411 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/92557d03bb12543040c8bb5f0475cbdd9968f05f/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2043] 2023-11-07 03:32:41,411 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/92557d03bb12543040c8bb5f0475cbdd9968f05f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2043] 2023-11-07 03:32:41,411 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/92557d03bb12543040c8bb5f0475cbdd9968f05f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2043] 2023-11-07 03:32:41,411 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/92557d03bb12543040c8bb5f0475cbdd9968f05f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2043] 2023-11-07 03:32:41,411 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/92557d03bb12543040c8bb5f0475cbdd9968f05f/tokenizer_config.json\n",
            "[WARNING|logging.py:290] 2023-11-07 03:32:41,475 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:715] 2023-11-07 03:32:41,518 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/92557d03bb12543040c8bb5f0475cbdd9968f05f/config.json\n",
            "[INFO|configuration_utils.py:715] 2023-11-07 03:32:41,620 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/92557d03bb12543040c8bb5f0475cbdd9968f05f/config.json\n",
            "[INFO|configuration_utils.py:775] 2023-11-07 03:32:41,621 >> Model config MixFormerSequentialConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"MixFormerSequentialForCausalLM\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/phi-1_5--configuration_mixformer_sequential.MixFormerSequentialConfig\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_mixformer_sequential.MixFormerSequentialForCausalLM\"\n",
            "  },\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"flash_rotary\": false,\n",
            "  \"fused_dense\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"mixformer-sequential\",\n",
            "  \"n_embd\": 2048,\n",
            "  \"n_head\": 32,\n",
            "  \"n_head_kv\": null,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 2048,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rotary_dim\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.34.0\",\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2993] 2023-11-07 03:32:41,713 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/92557d03bb12543040c8bb5f0475cbdd9968f05f/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1220] 2023-11-07 03:33:11,579 >> Instantiating MixFormerSequentialForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:770] 2023-11-07 03:33:11,744 >> Generate config GenerationConfig {}\n",
            "\n",
            "[INFO|modeling_utils.py:3775] 2023-11-07 03:33:46,006 >> All model checkpoint weights were used when initializing MixFormerSequentialForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:3783] 2023-11-07 03:33:46,007 >> All the weights of MixFormerSequentialForCausalLM were initialized from the model checkpoint at microsoft/phi-1_5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MixFormerSequentialForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:730] 2023-11-07 03:33:46,066 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/92557d03bb12543040c8bb5f0475cbdd9968f05f/generation_config.json\n",
            "[INFO|configuration_utils.py:770] 2023-11-07 03:33:46,066 >> Generate config GenerationConfig {}\n",
            "\n",
            "11/07/2023 03:33:46 - INFO - llmtuner.tuner.core.utils - Gradient checkpointing enabled.\n",
            "11/07/2023 03:33:46 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA\n",
            "11/07/2023 03:33:46 - INFO - llmtuner.tuner.core.loader - trainable params: 1572864 || all params: 1419843584 || trainable%: 0.1108\n",
            "11/07/2023 03:33:46 - INFO - llmtuner.extras.template - Add pad token: <|endoftext|>\n",
            "[INFO|tokenization_utils_base.py:952] 2023-11-07 03:33:46,113 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
            "Running tokenizer on dataset:   0% 0/1080 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:3839] 2023-11-07 03:33:46,313 >> Token indices sequence length is longer than the specified maximum sequence length for this model (3014 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-553a64b70ec753a7/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f69251a74cbb9389.arrow\n",
            "Running tokenizer on dataset: 100% 1080/1080 [00:02<00:00, 421.90 examples/s]\n",
            "input_ids:\n",
            "[50256, 19526, 254, 25001, 121, 162, 224, 101, 25001, 121, 171, 120, 234, 22755, 239, 42468, 10545, 111, 243, 36181, 233, 20185, 171, 120, 234, 31660, 10310, 103, 18796, 109, 2855, 14259, 1326, 10263, 120, 222, 20998, 239, 21410, 9552, 10263, 232, 102, 33699, 233, 171, 120, 234, 36181, 230, 165, 45865, 17739, 112, 164, 106, 97, 46237, 228, 162, 224, 101, 16764, 46237, 115, 29785, 106, 22755, 239, 47797, 121, 10310, 118, 162, 224, 101, 161, 223, 248, 12859, 249, 20015, 222, 20046, 230, 171, 120, 253, 50256]\n",
            "inputs:\n",
            "<|endoftext|>‰Ω†Â•ΩÊÇ®Â•ΩÔºåÊàëÊòØ Ê≥ïÂæãAIÔºå‰∏Ä‰∏™Áî± billvsme ÂºÄÂèëÁöÑ AI Âä©ÊâãÔºåÂæàÈ´òÂÖ¥ËÆ§ËØÜÊÇ®„ÄÇËØ∑ÈóÆÊàëËÉΩ‰∏∫ÊÇ®ÂÅö‰∫õ‰ªÄ‰πàÔºü<|endoftext|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, 162, 224, 101, 25001, 121, 171, 120, 234, 22755, 239, 42468, 10545, 111, 243, 36181, 233, 20185, 171, 120, 234, 31660, 10310, 103, 18796, 109, 2855, 14259, 1326, 10263, 120, 222, 20998, 239, 21410, 9552, 10263, 232, 102, 33699, 233, 171, 120, 234, 36181, 230, 165, 45865, 17739, 112, 164, 106, 97, 46237, 228, 162, 224, 101, 16764, 46237, 115, 29785, 106, 22755, 239, 47797, 121, 10310, 118, 162, 224, 101, 161, 223, 248, 12859, 249, 20015, 222, 20046, 230, 171, 120, 253, 50256]\n",
            "labels:\n",
            "ÊÇ®Â•ΩÔºåÊàëÊòØ Ê≥ïÂæãAIÔºå‰∏Ä‰∏™Áî± billvsme ÂºÄÂèëÁöÑ AI Âä©ÊâãÔºåÂæàÈ´òÂÖ¥ËÆ§ËØÜÊÇ®„ÄÇËØ∑ÈóÆÊàëËÉΩ‰∏∫ÊÇ®ÂÅö‰∫õ‰ªÄ‰πàÔºü<|endoftext|>\n",
            "[INFO|training_args.py:1345] 2023-11-07 03:33:48,714 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-07 03:33:48,714 >> PyTorch: setting up devices\n",
            "[INFO|deepspeed.py:303] 2023-11-07 03:33:49,112 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\n",
            "Building extension module cpu_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 2.947075605392456 seconds\n",
            "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
            "Config: alpha=0.000200, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
            "[2023-11-07 03:34:05,878] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.2, git-hash=unknown, git-branch=unknown\n",
            "[2023-11-07 03:34:07,661] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "[2023-11-07 03:34:07,663] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
            "[2023-11-07 03:34:07,663] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
            "[2023-11-07 03:34:07,667] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
            "[2023-11-07 03:34:07,667] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2023-11-07 03:34:07,667] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
            "[2023-11-07 03:34:07,667] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000\n",
            "[2023-11-07 03:34:07,667] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000\n",
            "[2023-11-07 03:34:07,667] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: True\n",
            "[2023-11-07 03:34:07,667] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False\n",
            "[2023-11-07 03:34:08,134] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states\n",
            "[2023-11-07 03:34:08,138] [INFO] [utils.py:803:see_memory_usage] MA 2.65 GB         Max_MA 2.65 GB         CA 2.69 GB         Max_CA 3 GB \n",
            "[2023-11-07 03:34:08,138] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 6.56 GB, percent = 51.8%\n",
            "[2023-11-07 03:34:08,924] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states\n",
            "[2023-11-07 03:34:08,926] [INFO] [utils.py:803:see_memory_usage] MA 2.65 GB         Max_MA 2.65 GB         CA 2.69 GB         Max_CA 3 GB \n",
            "[2023-11-07 03:34:08,926] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 6.62 GB, percent = 52.2%\n",
            "[2023-11-07 03:34:08,926] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized\n",
            "[2023-11-07 03:34:09,339] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2023-11-07 03:34:09,340] [INFO] [utils.py:803:see_memory_usage] MA 2.65 GB         Max_MA 2.65 GB         CA 2.69 GB         Max_CA 3 GB \n",
            "[2023-11-07 03:34:09,341] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 6.62 GB, percent = 52.2%\n",
            "[2023-11-07 03:34:09,344] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam\n",
            "[2023-11-07 03:34:09,344] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
            "[2023-11-07 03:34:09,344] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
            "[2023-11-07 03:34:09,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]\n",
            "[2023-11-07 03:34:09,346] [INFO] [config.py:972:print] DeepSpeedEngine configuration:\n",
            "[2023-11-07 03:34:09,347] [INFO] [config.py:976:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2023-11-07 03:34:09,347] [INFO] [config.py:976:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2023-11-07 03:34:09,347] [INFO] [config.py:976:print]   amp_enabled .................. False\n",
            "[2023-11-07 03:34:09,347] [INFO] [config.py:976:print]   amp_params ................... False\n",
            "[2023-11-07 03:34:09,348] [INFO] [config.py:976:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2023-11-07 03:34:09,348] [INFO] [config.py:976:print]   bfloat16_enabled ............. False\n",
            "[2023-11-07 03:34:09,348] [INFO] [config.py:976:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2023-11-07 03:34:09,348] [INFO] [config.py:976:print]   checkpoint_tag_validation_enabled  True\n",
            "[2023-11-07 03:34:09,348] [INFO] [config.py:976:print]   checkpoint_tag_validation_fail  False\n",
            "[2023-11-07 03:34:09,348] [INFO] [config.py:976:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7a10623a6110>\n",
            "[2023-11-07 03:34:09,348] [INFO] [config.py:976:print]   communication_data_type ...... None\n",
            "[2023-11-07 03:34:09,348] [INFO] [config.py:976:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2023-11-07 03:34:09,348] [INFO] [config.py:976:print]   curriculum_enabled_legacy .... False\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   curriculum_params_legacy ..... False\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   data_efficiency_enabled ...... False\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   dataloader_drop_last ......... False\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   disable_allgather ............ False\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   dump_state ................... False\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   eigenvalue_enabled ........... False\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   eigenvalue_layer_num ......... 0\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   eigenvalue_max_iter .......... 100\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   eigenvalue_stability ......... 1e-06\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   eigenvalue_tol ............... 0.01\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   eigenvalue_verbose ........... False\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   elasticity_enabled ........... False\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   fp16_auto_cast ............... False\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   fp16_enabled ................. True\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   fp16_master_weights_and_gradients  False\n",
            "[2023-11-07 03:34:09,349] [INFO] [config.py:976:print]   global_rank .................. 0\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   grad_accum_dtype ............. None\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   gradient_accumulation_steps .. 1\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   gradient_clipping ............ 1.0\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   gradient_predivide_factor .... 1.0\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   initial_dynamic_scale ........ 65536\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   load_universal_checkpoint .... False\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   loss_scale ................... 0\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   memory_breakdown ............. False\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   mics_hierarchial_params_gather  False\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   mics_shard_size .............. -1\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   optimizer_legacy_fusion ...... False\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   optimizer_name ............... None\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   optimizer_params ............. None\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   pld_enabled .................. False\n",
            "[2023-11-07 03:34:09,350] [INFO] [config.py:976:print]   pld_params ................... False\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   prescale_gradients ........... False\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   scheduler_name ............... None\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   scheduler_params ............. None\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   sparse_attention ............. None\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   sparse_gradients_enabled ..... False\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   steps_per_print .............. inf\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   train_batch_size ............. 1\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   train_micro_batch_size_per_gpu  1\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   use_node_local_storage ....... False\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   wall_clock_breakdown ......... False\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   weight_quantization_config ... None\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   world_size ................... 1\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   zero_allow_untested_optimizer  True\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   zero_enabled ................. True\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2023-11-07 03:34:09,351] [INFO] [config.py:976:print]   zero_optimization_stage ...... 2\n",
            "[2023-11-07 03:34:09,352] [INFO] [config.py:962:print_user_config]   json = {\n",
            "    \"train_batch_size\": 1, \n",
            "    \"train_micro_batch_size_per_gpu\": 1, \n",
            "    \"gradient_accumulation_steps\": 1, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"zero_allow_untested_optimizer\": true, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": true, \n",
            "        \"loss_scale\": 0, \n",
            "        \"initial_scale_power\": 16, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"hysteresis\": 2, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 2, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"allgather_partitions\": true, \n",
            "        \"allgather_bucket_size\": 5.000000e+08, \n",
            "        \"reduce_scatter\": true, \n",
            "        \"reduce_bucket_size\": 5.000000e+08, \n",
            "        \"overlap_comm\": false, \n",
            "        \"contiguous_gradients\": true\n",
            "    }, \n",
            "    \"steps_per_print\": inf, \n",
            "    \"bf16\": {\n",
            "        \"enabled\": false\n",
            "    }\n",
            "}\n",
            "[INFO|trainer.py:1760] 2023-11-07 03:34:09,352 >> ***** Running training *****\n",
            "[INFO|trainer.py:1761] 2023-11-07 03:34:09,352 >>   Num examples = 1,080\n",
            "[INFO|trainer.py:1762] 2023-11-07 03:34:09,352 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1763] 2023-11-07 03:34:09,352 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:1766] 2023-11-07 03:34:09,352 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1767] 2023-11-07 03:34:09,352 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1768] 2023-11-07 03:34:09,352 >>   Total optimization steps = 5,400\n",
            "[INFO|trainer.py:1769] 2023-11-07 03:34:09,354 >>   Number of trainable parameters = 1,572,864\n",
            "  0% 0/5400 [00:00<?, ?it/s][WARNING|logging.py:290] 2023-11-07 03:34:09,415 >> You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py:1881: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
            "{'loss': 1.4347, 'learning_rate': 0.00019999957692054863, 'epoch': 0.0}\n",
            "{'loss': 1.1958, 'learning_rate': 0.00019999830768577443, 'epoch': 0.01}\n",
            "{'loss': 1.4181, 'learning_rate': 0.00019999619230641713, 'epoch': 0.01}\n",
            "{'loss': 1.2049, 'learning_rate': 0.00019999323080037624, 'epoch': 0.02}\n",
            "{'loss': 1.2146, 'learning_rate': 0.00019998942319271077, 'epoch': 0.02}\n",
            "{'loss': 1.0214, 'learning_rate': 0.00019998476951563915, 'epoch': 0.03}\n",
            "{'loss': 1.4484, 'learning_rate': 0.00019997926980853885, 'epoch': 0.03}\n",
            "{'loss': 1.2273, 'learning_rate': 0.00019997292411794618, 'epoch': 0.04}\n",
            "{'loss': 0.8605, 'learning_rate': 0.00019996573249755572, 'epoch': 0.04}\n",
            "{'loss': 1.1234, 'learning_rate': 0.0001999576950082201, 'epoch': 0.05}\n",
            "{'loss': 0.8851, 'learning_rate': 0.0001999488117179491, 'epoch': 0.05}\n",
            "{'loss': 0.8706, 'learning_rate': 0.0001999390827019096, 'epoch': 0.06}\n",
            "{'loss': 0.7599, 'learning_rate': 0.00019992850804242447, 'epoch': 0.06}\n",
            "{'loss': 0.7714, 'learning_rate': 0.00019991708782897213, 'epoch': 0.06}\n",
            "{'loss': 0.7991, 'learning_rate': 0.0001999048221581858, 'epoch': 0.07}\n",
            "{'loss': 0.7849, 'learning_rate': 0.0001998917111338525, 'epoch': 0.07}\n",
            "{'loss': 0.9053, 'learning_rate': 0.00019987775486691228, 'epoch': 0.08}\n",
            "{'loss': 1.1732, 'learning_rate': 0.0001998629534754574, 'epoch': 0.08}\n",
            "{'loss': 0.7897, 'learning_rate': 0.00019984730708473114, 'epoch': 0.09}\n",
            "{'loss': 0.8607, 'learning_rate': 0.00019983081582712685, 'epoch': 0.09}\n",
            "{'loss': 0.699, 'learning_rate': 0.0001998134798421867, 'epoch': 0.1}\n",
            "{'loss': 0.7331, 'learning_rate': 0.00019979529927660074, 'epoch': 0.1}\n",
            "{'loss': 0.6408, 'learning_rate': 0.00019977627428420543, 'epoch': 0.11}\n",
            "{'loss': 0.6767, 'learning_rate': 0.00019975640502598244, 'epoch': 0.11}\n",
            "{'loss': 0.7336, 'learning_rate': 0.00019973569167005723, 'epoch': 0.12}\n",
            "{'loss': 0.7994, 'learning_rate': 0.00019971413439169775, 'epoch': 0.12}\n",
            "{'loss': 0.7359, 'learning_rate': 0.0001996917333733128, 'epoch': 0.12}\n",
            "{'loss': 0.7384, 'learning_rate': 0.00019966848880445062, 'epoch': 0.13}\n",
            "{'loss': 0.7645, 'learning_rate': 0.00019964440088179717, 'epoch': 0.13}\n",
            "{'loss': 0.7067, 'learning_rate': 0.00019961946980917456, 'epoch': 0.14}\n",
            "{'loss': 0.6167, 'learning_rate': 0.0001995936957975393, 'epoch': 0.14}\n",
            "{'loss': 0.571, 'learning_rate': 0.00019956707906498044, 'epoch': 0.15}\n",
            "{'loss': 0.6002, 'learning_rate': 0.00019953961983671788, 'epoch': 0.15}\n",
            "{'loss': 0.9193, 'learning_rate': 0.00019951131834510032, 'epoch': 0.16}\n",
            "{'loss': 0.692, 'learning_rate': 0.0001994821748296033, 'epoch': 0.16}\n",
            "  3% 175/5400 [02:37<1:24:49,  1.03it/s][2023-11-07 03:36:46,667] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
            "{'loss': 1.445, 'learning_rate': 0.0001994582539254887, 'epoch': 0.17}\n",
            "{'loss': 0.7888, 'learning_rate': 0.00019942759539322844, 'epoch': 0.17}\n",
            "{'loss': 0.6851, 'learning_rate': 0.000199396095545518, 'epoch': 0.18}\n",
            "{'loss': 0.7159, 'learning_rate': 0.00019936375464889606, 'epoch': 0.18}\n",
            "{'loss': 0.6715, 'learning_rate': 0.00019933057297701808, 'epoch': 0.19}\n",
            "{'loss': 0.6105, 'learning_rate': 0.0001992965508106537, 'epoch': 0.19}\n",
            "{'loss': 0.7572, 'learning_rate': 0.00019926168843768452, 'epoch': 0.19}\n",
            "{'loss': 1.0751, 'learning_rate': 0.00019922598615310157, 'epoch': 0.2}\n",
            "{'loss': 0.6948, 'learning_rate': 0.000199189444259003, 'epoch': 0.2}\n",
            "{'loss': 0.7809, 'learning_rate': 0.00019915206306459117, 'epoch': 0.21}\n",
            "{'loss': 0.6371, 'learning_rate': 0.0001991138428861705, 'epoch': 0.21}\n",
            "{'loss': 0.7681, 'learning_rate': 0.00019907478404714436, 'epoch': 0.22}\n",
            "{'loss': 0.6309, 'learning_rate': 0.00019903488687801266, 'epoch': 0.22}\n",
            "{'loss': 0.6515, 'learning_rate': 0.0001989941517163688, 'epoch': 0.23}\n",
            "{'loss': 0.8847, 'learning_rate': 0.00019895257890689696, 'epoch': 0.23}\n",
            "{'loss': 0.5608, 'learning_rate': 0.0001989101688013692, 'epoch': 0.24}\n",
            "{'loss': 0.5855, 'learning_rate': 0.00019886692175864244, 'epoch': 0.24}\n",
            "{'loss': 0.6974, 'learning_rate': 0.0001988228381446553, 'epoch': 0.25}\n",
            "{'loss': 0.5773, 'learning_rate': 0.00019877791833242524, 'epoch': 0.25}\n",
            "{'loss': 0.6934, 'learning_rate': 0.0001987321627020453, 'epoch': 0.25}\n",
            "{'loss': 0.6712, 'learning_rate': 0.00019868557164068074, 'epoch': 0.26}\n",
            "{'loss': 0.8014, 'learning_rate': 0.00019863814554256601, 'epoch': 0.26}\n",
            "{'loss': 0.8451, 'learning_rate': 0.00019858988480900126, 'epoch': 0.27}\n",
            "{'loss': 0.5892, 'learning_rate': 0.00019854078984834903, 'epoch': 0.27}\n",
            "{'loss': 0.7892, 'learning_rate': 0.0001984908610760306, 'epoch': 0.28}\n",
            "{'loss': 0.5856, 'learning_rate': 0.0001984400989145228, 'epoch': 0.28}\n",
            "{'loss': 0.5554, 'learning_rate': 0.00019838850379335417, 'epoch': 0.29}\n",
            "{'loss': 0.9167, 'learning_rate': 0.0001983360761491014, 'epoch': 0.29}\n",
            "{'loss': 0.4403, 'learning_rate': 0.00019828281642538567, 'epoch': 0.3}\n",
            "{'loss': 0.6196, 'learning_rate': 0.0001982287250728689, 'epoch': 0.3}\n",
            "{'loss': 0.8228, 'learning_rate': 0.00019817380254924982, 'epoch': 0.31}\n",
            "{'loss': 1.2751, 'learning_rate': 0.00019811804931926032, 'epoch': 0.31}\n",
            "{'loss': 0.6121, 'learning_rate': 0.0001980614658546613, 'epoch': 0.31}\n",
            "{'loss': 0.8009, 'learning_rate': 0.0001980040526342388, 'epoch': 0.32}\n",
            "{'loss': 0.6016, 'learning_rate': 0.00019794581014379988, 'epoch': 0.32}\n",
            "{'loss': 0.6426, 'learning_rate': 0.0001978867388761685, 'epoch': 0.33}\n",
            "{'loss': 1.272, 'learning_rate': 0.00019782683933118156, 'epoch': 0.33}\n",
            "{'loss': 1.0474, 'learning_rate': 0.00019776611201568432, 'epoch': 0.34}\n",
            "{'loss': 0.6331, 'learning_rate': 0.0001977045574435264, 'epoch': 0.34}\n",
            "{'loss': 0.8054, 'learning_rate': 0.00019764217613555722, 'epoch': 0.35}\n",
            "{'loss': 0.4934, 'learning_rate': 0.00019757896861962189, 'epoch': 0.35}\n",
            "{'loss': 0.6083, 'learning_rate': 0.00019751493543055632, 'epoch': 0.36}\n",
            "{'loss': 0.5655, 'learning_rate': 0.00019745007711018312, 'epoch': 0.36}\n",
            "{'loss': 0.632, 'learning_rate': 0.0001973843942073067, 'epoch': 0.37}\n",
            "{'loss': 0.5184, 'learning_rate': 0.00019731788727770885, 'epoch': 0.37}\n",
            "{'loss': 0.5196, 'learning_rate': 0.00019725055688414378, 'epoch': 0.38}\n",
            "{'loss': 0.5785, 'learning_rate': 0.00019718240359633366, 'epoch': 0.38}\n",
            "{'loss': 0.6278, 'learning_rate': 0.00019711342799096361, 'epoch': 0.38}\n",
            "{'loss': 0.6185, 'learning_rate': 0.00019704363065167684, 'epoch': 0.39}\n",
            "{'loss': 0.5014, 'learning_rate': 0.0001969730121690698, 'epoch': 0.39}\n",
            "{'loss': 0.4497, 'learning_rate': 0.00019690157314068696, 'epoch': 0.4}\n",
            "{'loss': 0.5317, 'learning_rate': 0.00019682931417101612, 'epoch': 0.4}\n",
            "{'loss': 0.5174, 'learning_rate': 0.00019675623587148293, 'epoch': 0.41}\n",
            "{'loss': 0.672, 'learning_rate': 0.00019668233886044597, 'epoch': 0.41}\n",
            "{'loss': 0.5851, 'learning_rate': 0.0001966076237631913, 'epoch': 0.42}\n",
            "{'loss': 0.7721, 'learning_rate': 0.00019653209121192746, 'epoch': 0.42}\n",
            "{'loss': 0.4987, 'learning_rate': 0.00019645574184577982, 'epoch': 0.43}\n",
            "{'loss': 0.5974, 'learning_rate': 0.00019637857631078532, 'epoch': 0.43}\n",
            "{'loss': 0.6418, 'learning_rate': 0.00019630059525988704, 'epoch': 0.44}\n",
            "{'loss': 0.4988, 'learning_rate': 0.00019622179935292855, 'epoch': 0.44}\n",
            "{'loss': 0.6074, 'learning_rate': 0.00019614218925664845, 'epoch': 0.44}\n",
            "{'loss': 0.7663, 'learning_rate': 0.00019606176564467465, 'epoch': 0.45}\n",
            "{'loss': 0.4568, 'learning_rate': 0.0001959805291975187, 'epoch': 0.45}\n",
            "{'loss': 0.6092, 'learning_rate': 0.00019589848060257004, 'epoch': 0.46}\n",
            "{'loss': 0.5471, 'learning_rate': 0.00019581562055409016, 'epoch': 0.46}\n",
            "{'loss': 0.6243, 'learning_rate': 0.00019573194975320673, 'epoch': 0.47}\n",
            "{'loss': 0.4525, 'learning_rate': 0.0001956474689079077, 'epoch': 0.47}\n",
            "{'loss': 0.4486, 'learning_rate': 0.00019556217873303524, 'epoch': 0.48}\n",
            "{'loss': 0.4944, 'learning_rate': 0.00019547607995027978, 'epoch': 0.48}\n",
            "{'loss': 0.5014, 'learning_rate': 0.00019538917328817376, 'epoch': 0.49}\n",
            "{'loss': 0.5143, 'learning_rate': 0.00019530145948208575, 'epoch': 0.49}\n",
            "{'loss': 0.6413, 'learning_rate': 0.00019521293927421388, 'epoch': 0.5}\n",
            "{'loss': 0.705, 'learning_rate': 0.00019512361341357976, 'epoch': 0.5}\n",
            "{'loss': 0.5385, 'learning_rate': 0.00019503348265602214, 'epoch': 0.5}\n",
            "{'loss': 0.5996, 'learning_rate': 0.0001949425477641904, 'epoch': 0.51}\n",
            "{'loss': 0.5249, 'learning_rate': 0.00019485080950753828, 'epoch': 0.51}\n",
            "{'loss': 0.4281, 'learning_rate': 0.00019475826866231715, 'epoch': 0.52}\n",
            "{'loss': 0.631, 'learning_rate': 0.00019466492601156966, 'epoch': 0.52}\n",
            "{'loss': 0.5031, 'learning_rate': 0.0001945707823451229, 'epoch': 0.53}\n",
            "{'loss': 0.4569, 'learning_rate': 0.00019447583845958198, 'epoch': 0.53}\n",
            "{'loss': 0.4263, 'learning_rate': 0.00019438009515832297, 'epoch': 0.54}\n",
            "{'loss': 0.5315, 'learning_rate': 0.00019428355325148633, 'epoch': 0.54}\n",
            "{'loss': 0.5607, 'learning_rate': 0.00019418621355597003, 'epoch': 0.55}\n",
            "{'loss': 0.5175, 'learning_rate': 0.00019408807689542257, 'epoch': 0.55}\n",
            "{'loss': 0.4899, 'learning_rate': 0.00019398914410023602, 'epoch': 0.56}\n",
            "{'loss': 0.4673, 'learning_rate': 0.00019388941600753903, 'epoch': 0.56}\n",
            "{'loss': 0.5933, 'learning_rate': 0.0001937888934611898, 'epoch': 0.56}\n",
            "{'loss': 0.536, 'learning_rate': 0.0001936875773117687, 'epoch': 0.57}\n",
            "{'loss': 0.5918, 'learning_rate': 0.00019358546841657142, 'epoch': 0.57}\n",
            "{'loss': 0.5395, 'learning_rate': 0.00019348256763960145, 'epoch': 0.58}\n",
            "{'loss': 0.5372, 'learning_rate': 0.0001933788758515629, 'epoch': 0.58}\n",
            "{'loss': 0.4192, 'learning_rate': 0.000193274393929853, 'epoch': 0.59}\n",
            "{'loss': 0.5918, 'learning_rate': 0.0001931691227585549, 'epoch': 0.59}\n",
            "{'loss': 0.4379, 'learning_rate': 0.00019306306322842994, 'epoch': 0.6}\n",
            "{'loss': 0.6036, 'learning_rate': 0.00019295621623691034, 'epoch': 0.6}\n",
            "{'loss': 0.5661, 'learning_rate': 0.00019284858268809137, 'epoch': 0.61}\n",
            "{'loss': 0.5676, 'learning_rate': 0.0001927401634927239, 'epoch': 0.61}\n",
            "{'loss': 0.6538, 'learning_rate': 0.0001926309595682066, 'epoch': 0.62}\n",
            "{'loss': 0.4317, 'learning_rate': 0.00019252097183857823, 'epoch': 0.62}\n",
            "{'loss': 0.6099, 'learning_rate': 0.0001924102012345097, 'epoch': 0.62}\n",
            "{'loss': 0.5144, 'learning_rate': 0.0001922986486932964, 'epoch': 0.63}\n",
            "{'loss': 0.6607, 'learning_rate': 0.00019218631515885006, 'epoch': 0.63}\n",
            "{'loss': 0.4286, 'learning_rate': 0.0001920732015816909, 'epoch': 0.64}\n",
            "{'loss': 0.6475, 'learning_rate': 0.00019195930891893946, 'epoch': 0.64}\n",
            "{'loss': 0.5175, 'learning_rate': 0.00019184463813430873, 'epoch': 0.65}\n",
            "{'loss': 0.6173, 'learning_rate': 0.0001917291901980957, 'epoch': 0.65}\n",
            "{'loss': 0.523, 'learning_rate': 0.00019161296608717342, 'epoch': 0.66}\n",
            "{'loss': 0.6231, 'learning_rate': 0.0001914959667849825, 'epoch': 0.66}\n",
            "{'loss': 0.6144, 'learning_rate': 0.00019137819328152294, 'epoch': 0.67}\n",
            "{'loss': 0.5384, 'learning_rate': 0.00019125964657334577, 'epoch': 0.67}\n",
            "{'loss': 0.5745, 'learning_rate': 0.00019114032766354453, 'epoch': 0.68}\n",
            "{'loss': 0.4298, 'learning_rate': 0.00019102023756174675, 'epoch': 0.68}\n",
            "{'loss': 0.4423, 'learning_rate': 0.00019089937728410553, 'epoch': 0.69}\n",
            "{'loss': 0.5091, 'learning_rate': 0.00019077774785329087, 'epoch': 0.69}\n",
            "{'loss': 0.4185, 'learning_rate': 0.00019065535029848106, 'epoch': 0.69}\n",
            "{'loss': 0.5597, 'learning_rate': 0.0001905321856553538, 'epoch': 0.7}\n",
            "{'loss': 0.5429, 'learning_rate': 0.00019040825496607786, 'epoch': 0.7}\n",
            "{'loss': 0.4793, 'learning_rate': 0.00019028355927930364, 'epoch': 0.71}\n",
            "{'loss': 0.4633, 'learning_rate': 0.00019015809965015489, 'epoch': 0.71}\n",
            "{'loss': 0.453, 'learning_rate': 0.00019003187714021938, 'epoch': 0.72}\n",
            "{'loss': 0.5522, 'learning_rate': 0.0001899048928175401, 'epoch': 0.72}\n",
            "{'loss': 0.4472, 'learning_rate': 0.0001897771477566063, 'epoch': 0.73}\n",
            "{'loss': 0.4852, 'learning_rate': 0.00018964864303834406, 'epoch': 0.73}\n",
            "{'loss': 0.478, 'learning_rate': 0.00018951937975010758, 'epoch': 0.74}\n",
            "{'loss': 0.5887, 'learning_rate': 0.00018938935898566965, 'epoch': 0.74}\n",
            "{'loss': 0.5042, 'learning_rate': 0.00018925858184521256, 'epoch': 0.75}\n",
            "{'loss': 0.4578, 'learning_rate': 0.0001891270494353187, 'epoch': 0.75}\n",
            "{'loss': 0.4039, 'learning_rate': 0.0001889947628689613, 'epoch': 0.75}\n",
            "{'loss': 0.4716, 'learning_rate': 0.0001888617232654949, 'epoch': 0.76}\n",
            "{'loss': 0.4776, 'learning_rate': 0.00018872793175064593, 'epoch': 0.76}\n",
            "{'loss': 0.5313, 'learning_rate': 0.00018859338945650324, 'epoch': 0.77}\n",
            "{'loss': 0.4684, 'learning_rate': 0.0001884580975215084, 'epoch': 0.77}\n",
            "{'loss': 0.5863, 'learning_rate': 0.00018832205709044618, 'epoch': 0.78}\n",
            "{'loss': 0.4934, 'learning_rate': 0.0001881852693144348, 'epoch': 0.78}\n",
            "{'loss': 0.549, 'learning_rate': 0.0001880477353509162, 'epoch': 0.79}\n",
            "{'loss': 0.4801, 'learning_rate': 0.00018790945636364627, 'epoch': 0.79}\n",
            "{'loss': 0.5339, 'learning_rate': 0.00018777043352268493, 'epoch': 0.8}\n",
            "{'loss': 0.5875, 'learning_rate': 0.00018763066800438636, 'epoch': 0.8}\n",
            "{'loss': 0.5745, 'learning_rate': 0.00018749016099138893, 'epoch': 0.81}\n",
            "{'loss': 0.5057, 'learning_rate': 0.00018734891367260526, 'epoch': 0.81}\n",
            "{'loss': 0.4643, 'learning_rate': 0.00018720692724321207, 'epoch': 0.81}\n",
            "{'loss': 0.6939, 'learning_rate': 0.0001870642029046402, 'epoch': 0.82}\n",
            "{'loss': 0.5437, 'learning_rate': 0.00018692074186456433, 'epoch': 0.82}\n",
            " 17% 892/5400 [13:54<1:21:59,  1.09s/it]"
          ]
        }
      ],
      "source": [
        "\"\"\"ËÆ≠ÁªÉ\n",
        "Êåá‰ª§ÁõëÁù£ÂæÆË∞ÉÔºåloraÊñπÂºèÔºå‰ΩøÁî®self_cognitionÂíålaw_sft_tripletÂâç1000Êï∞ÊçÆÈõÜÔºåÂ§ßÊ¶ÇÈúÄË¶Å60ÂàÜÈíü\n",
        "\n",
        "Áî±‰∫éÊ≤°ÊúâÂØπËØùÂéÜÂè≤Ôºåtemplate‰ΩøÁî®vanilla\n",
        "\n",
        "‰ΩøÁî®deepspeed stage2Ôºåoffload_optimizer -> cpuËäÇÁúÅÊòæÂ≠òÔºåcutoff_len ÂèØ‰ª•Âà∞1664ÔºåÂú®Â§öÂ∞±Ë¶ÅÁàÜÊòæÂ≠ò‰∫Ü\n",
        "\"\"\"\n",
        "%cd /content/LLaMA-Factory\n",
        "!rm -rf saves/Phi1.5-1.3B/lora/law\n",
        "!deepspeed --num_gpus 1 --master_port=9901 src/train_bash.py \\\n",
        "    --deepspeed ds_config.json \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/phi-1_5 \\\n",
        "    --do_train True \\\n",
        "    --finetuning_type lora \\\n",
        "    --template vanilla \\\n",
        "    --flash_attn False \\\n",
        "    --shift_attn False \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset self_cognition,law_sft_triplet \\\n",
        "    --cutoff_len 1664 \\\n",
        "    --learning_rate 2e-04 \\\n",
        "    --num_train_epochs 5.0 \\\n",
        "    --max_samples 1000 \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --neft_alpha 0 \\\n",
        "    --train_on_prompt False \\\n",
        "    --upcast_layernorm False \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_dropout 0.1 \\\n",
        "    --lora_target Wqkv \\\n",
        "    --resume_lora_training True \\\n",
        "    --output_dir saves/Phi1.5-1.3B/lora/law \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr7sVbFoElmt"
      },
      "outputs": [],
      "source": [
        "\"\"\"ÂØºÂá∫Ê®°Âûã\n",
        "\"\"\"\n",
        "%cd /content/LLaMA-Factory\n",
        "!mkdir out_model\n",
        "!python src/export_model.py \\\n",
        "    --model_name_or_path  microsoft/phi-1_5 \\\n",
        "    --template default \\\n",
        "    --finetuning_type lora \\\n",
        "    --checkpoint_dir saves/Phi1.5-1.3B/lora/law \\\n",
        "    --export_dir out_model/law"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkfPWnYOERj1"
      },
      "outputs": [],
      "source": [
        "\"\"\"Êü•ÁúãÊ®°ÂûãÊïàÊûú\n",
        "\"\"\"\n",
        "\n",
        "%cd /content/LLaMA-Factory\n",
        "\n",
        "import os\n",
        "from threading import Thread\n",
        "\n",
        "import torch\n",
        "from transformers.generation.streamers import TextIteratorStreamer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "\n",
        "torch.set_default_device(\"cuda\")\n",
        "\n",
        "model_name = \"/content/LLaMA-Factory/out_model/law/\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
        "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def stream(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, return_attention_mask=True)\n",
        "    streamer = TextIteratorStreamer(\n",
        "        tokenizer,\n",
        "        decode_kwargs={\"skip_special_tokens\": True})\n",
        "    Thread(\n",
        "        target=model.generate, kwargs=dict(\n",
        "            inputs, streamer=streamer,\n",
        "            max_new_tokens=256)\n",
        "    ).start()\n",
        "\n",
        "    first = True\n",
        "    for text in streamer:\n",
        "        if first and text:\n",
        "            first = False\n",
        "            continue\n",
        "\n",
        "        if not text:\n",
        "            continue\n",
        "        if tokenizer.eos_token in text:\n",
        "            break\n",
        "\n",
        "        yield text\n",
        "\n",
        "\n",
        "def main():\n",
        "    welcome_prompt = \"Ê¨¢Ëøé‰ΩøÁî® Ê≥ïÂæãAI Ê®°ÂûãÔºåËæìÂÖ•ÂÜÖÂÆπÂç≥ÂèØËøõË°åÂØπËØùÔºåclear Ê∏ÖÁ©∫ÂØπËØùÂéÜÂè≤Ôºåstop ÁªàÊ≠¢Á®ãÂ∫è\"\n",
        "    print(welcome_prompt)\n",
        "    while True:\n",
        "        query = input(\"\\nÁî®Êà∑Ôºö\")\n",
        "        if query.strip() == \"stop\":\n",
        "            break\n",
        "        if query.strip() == \"clear\":\n",
        "            os.system(\"clr\")\n",
        "            print(welcome_prompt)\n",
        "            continue\n",
        "        print(\"\\nÊ≥ïÂæãAIÔºö\", end=\"\")\n",
        "        for text in stream(query):\n",
        "            print(text, end=\"\", flush=True)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"‰øùÂ≠òËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÂà∞Google‰∫ëÁõò\n",
        "\"\"\"\n",
        "%cd /content\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!mkdir /content/drive/MyDrive/llm_model/\n",
        "!cp -r /content/LLaMA-Factory/out_model/law /content/drive/MyDrive/llm_model/"
      ],
      "metadata": {
        "id": "YKqWoT7m0hJ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}