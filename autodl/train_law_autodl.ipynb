{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BdJmJPb07U47",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"下载\n",
    "LLaMA-Factory 用于微调\n",
    "DISC-Law-SFT 法律数据\n",
    "\"\"\"\n",
    "%cd /root/autodl-tmp\n",
    "!source /etc/network_turbo && git clone --depth=1 https://github.com/hiyouga/LLaMA-Factory\n",
    "!source /etc/network_turbo &&  git clone https://huggingface.co/datasets/ShengbinYue/DISC-Law-SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"下载模型\n",
    "\"\"\"\n",
    "%cd /root/autodl-tmp\n",
    "!source /etc/network_turbo && git clone --depth=1 https://huggingface.co/THUDM/chatglm3-6b\n",
    "!source /etc/network_turbo && git clone --depth=1 https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat\n",
    "!source /etc/network_turbo && git clone --depth=1 https://huggingface.co/microsoft/phi-1_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o55DdL4NcS9E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"在LLaMA-Factory中添加DISC-Law-SFT 法律数据\n",
    "\"\"\"\n",
    "%cd /root/autodl-tmp\n",
    "!apt-get install -y jq\n",
    "!cp DISC-Law-SFT/DISC-Law-SFT-Pair.jsonl LLaMA-Factory/data/\n",
    "!cp DISC-Law-SFT/DISC-Law-SFT-Triplet-released.jsonl LLaMA-Factory/data/\n",
    "!jq '.law_sft_pair={\"file_name\": \"DISC-Law-SFT-Pair.jsonl\", \"columns\": {\"prompt\": \"input\", \"response\": \"output\"}}' LLaMA-Factory/data/dataset_info.json > new_dataset_info.json\n",
    "!cp  new_dataset_info.json LLaMA-Factory/data/dataset_info.json\n",
    "!jq '.law_sft_triplet={\"file_name\": \"DISC-Law-SFT-Triplet-released.jsonl\", \"columns\": {\"prompt\": \"input\", \"response\": \"output\"}}' LLaMA-Factory/data/dataset_info.json > new_dataset_info.json\n",
    "!cp  new_dataset_info.json LLaMA-Factory/data/dataset_info.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"替换自我认知self_cognition数据集中的名称\n",
    "\"\"\"\n",
    "%cd /root/autodl-tmp/LLaMA-Factory\n",
    "!sed -i 's/<NAME>/法律AI/g' data/self_cognition.json\n",
    "!sed -i 's/<AUTHOR>/billvsme/g' data/self_cognition.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h43G1zhf7msU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"安装依赖\n",
    "\"\"\"\n",
    "%cd /root/autodl-tmp/LLaMA-Factory\n",
    "!pip install -r requirements.txt\n",
    "!pip install einops\n",
    "!pip install transformers==4.34.0\n",
    "!pip install deepspeed\n",
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSf1qpsoW7cR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"生成deepspeed配置文件\n",
    "stage 2\n",
    "\"\"\"\n",
    "%cd /root/autodl-tmp/LLaMA-Factory\n",
    "!echo '''{\\\n",
    "  \"train_batch_size\": \"auto\",\\\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\\\n",
    "  \"gradient_accumulation_steps\": \"auto\",\\\n",
    "  \"gradient_clipping\": \"auto\",\\\n",
    "  \"zero_allow_untested_optimizer\": true,\\\n",
    "  \"fp16\": {\\\n",
    "    \"enabled\": \"auto\",\\\n",
    "    \"loss_scale\": 0,\\\n",
    "    \"initial_scale_power\": 16,\\\n",
    "    \"loss_scale_window\": 1000,\\\n",
    "    \"hysteresis\": 2,\\\n",
    "    \"min_loss_scale\": 1\\\n",
    "  },\\\n",
    "  \"zero_optimization\": {\\\n",
    "    \"stage\": 2,\\\n",
    "    \"offload_optimizer\": {\\\n",
    "      \"device\": \"cpu\",\\\n",
    "      \"pin_memory\": true\\\n",
    "    },\\\n",
    "    \"allgather_partitions\": true,\\\n",
    "    \"allgather_bucket_size\": 2e8,\\\n",
    "    \"reduce_scatter\": true,\\\n",
    "    \"reduce_bucket_size\":2e8,\\\n",
    "    \"overlap_comm\": true,\\\n",
    "    \"contiguous_gradients\": true\\\n",
    "  }\\\n",
    "}''' > ds_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"生成deepspeed配置文件\n",
    "stage 3\n",
    "\"\"\"\n",
    "%cd /root/autodl-tmp/LLaMA-Factory\n",
    "!echo '''{\\\n",
    "  \"train_batch_size\": \"auto\",\\\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\\\n",
    "  \"gradient_accumulation_steps\": \"auto\",\\\n",
    "  \"gradient_clipping\": \"auto\",\\\n",
    "  \"zero_allow_untested_optimizer\": true,\\\n",
    "  \"fp16\": {\\\n",
    "    \"enabled\": \"auto\",\\\n",
    "    \"loss_scale\": 0,\\\n",
    "    \"initial_scale_power\": 16,\\\n",
    "    \"loss_scale_window\": 1000,\\\n",
    "    \"hysteresis\": 2,\\\n",
    "    \"min_loss_scale\": 1\\\n",
    "  },\\\n",
    "  \"zero_optimization\": {\\\n",
    "    \"stage\": 3,\\\n",
    "    \"offload_optimizer\": {\\\n",
    "      \"device\": \"cpu\",\\\n",
    "      \"pin_memory\": true\\\n",
    "    },\\\n",
    "    \"offload_param\": {\\\n",
    "      \"device\": \"cpu\",\\\n",
    "      \"pin_memory\": true\\\n",
    "    },\\\n",
    "    \"overlap_comm\": true,\\\n",
    "    \"contiguous_gradients\": true,\\\n",
    "    \"sub_group_size\": 5e7,\\\n",
    "    \"reduce_bucket_size\": \"auto\",\\\n",
    "    \"stage3_prefetch_bucket_size\": \"auto\",\\\n",
    "    \"stage3_param_persistence_threshold\": \"auto\",\\\n",
    "    \"stage3_max_live_parameters\": 5e7,\\\n",
    "    \"stage3_max_reuse_distance\": 5e7,\\\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": true\\\n",
    "  }\\\n",
    "}''' > ds_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZmSHoI_8Jo6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"phi-1_5 训练\n",
    "指令监督微调，lora方式，使用self_cognition\n",
    "\n",
    "由于没有对话历史，template使用chatglm3\n",
    "\n",
    "使用deepspeed stage2，offload_optimizer -> cpu节省显存\n",
    "\"\"\"\n",
    "%cd /root/autodl-tmp/LLaMA-Factory\n",
    "!rm -rf saves/Phi1.5-1.3B/full/law_full\n",
    "!deepspeed --num_gpus 2 --master_port=9901 src/train_bash.py \\\n",
    "    --deepspeed ds_config.json \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path ../phi-1_5/ \\\n",
    "    --do_train True \\\n",
    "    --finetuning_type full \\\n",
    "    --template vanilla \\\n",
    "    --flash_attn False \\\n",
    "    --shift_attn False \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset self_cognition,law_sft_triplet \\\n",
    "    --cutoff_len 2048 \\\n",
    "    --learning_rate 2e-04 \\\n",
    "    --num_train_epochs 5.0 \\\n",
    "    --max_samples 1000 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --neft_alpha 0 \\\n",
    "    --train_on_prompt False \\\n",
    "    --upcast_layernorm False \\\n",
    "    --output_dir saves/Phi1.5-1.3B/full/law_full \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZmSHoI_8Jo6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"chatglm3-6b 训练 lora\n",
    "指令监督微调，lora方式，使用self_cognition\n",
    "\n",
    "由于没有对话历史，template使用chatglm3\n",
    "\n",
    "使用deepspeed stage2，offload_optimizer -> cpu节省显存\n",
    "\"\"\"\n",
    "%cd /root/autodl-tmp/LLaMA-Factory\n",
    "!rm -rf saves/chatglm3/lora/law\n",
    "!source /etc/network_turbo && deepspeed --num_gpus 2 --master_port=9901 src/train_bash.py \\\n",
    "    --deepspeed ds_config.json \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path ../chatglm3-6b \\\n",
    "    --do_train True \\\n",
    "    --finetuning_type lora \\\n",
    "    --template chatglm3 \\\n",
    "    --flash_attn False \\\n",
    "    --shift_attn False \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset self_cognition,law_sft_triplet \\\n",
    "    --cutoff_len 2048 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 10.0 \\\n",
    "    --max_samples 1000 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --neft_alpha 0 \\\n",
    "    --train_on_prompt False \\\n",
    "    --upcast_layernorm False \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target query_key_value \\\n",
    "    --resume_lora_training True \\\n",
    "    --output_dir saves/chatglm3/lora/law \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZmSHoI_8Jo6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"chatglm3-6b 训练 full\n",
    "指令监督微调，full方式，使用self_cognition\n",
    "\n",
    "由于没有对话历史，template使用chatglm3\n",
    "\n",
    "使用deepspeed stage2，offload_optimizer -> cpu节省显存\n",
    "\"\"\"\n",
    "%cd /root/autodl-tmp/LLaMA-Factory\n",
    "!rm -rf saves/chatglm3/full/law\n",
    "!source /etc/network_turbo && deepspeed --num_gpus 2 --master_port=9901 src/train_bash.py \\\n",
    "    --deepspeed ds_config.json \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path ../chatglm3-6b \\\n",
    "    --do_train True \\\n",
    "    --finetuning_type full \\\n",
    "    --template chatglm3 \\\n",
    "    --flash_attn False \\\n",
    "    --shift_attn False \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset self_cognition \\\n",
    "    --cutoff_len 2048 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 15.0 \\\n",
    "    --max_samples 1000 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --neft_alpha 0 \\\n",
    "    --train_on_prompt False \\\n",
    "    --upcast_layernorm False \\\n",
    "    --output_dir saves/chatglm3/full/law \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZmSHoI_8Jo6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Baichuan2 训练 lora\n",
    "指令监督微调，lora方式，使用self_cognition\n",
    "\n",
    "由于没有对话历史，template使用chatglm3\n",
    "\n",
    "使用deepspeed stage2，offload_optimizer -> cpu节省显存\n",
    "\"\"\"\n",
    "%cd /root/autodl-tmp/LLaMA-Factory\n",
    "!pip install transformers==4.33.1\n",
    "!pip install bitsandbytes\n",
    "!rm -rf saves/chatglm3/lora/law\n",
    "!source /etc/network_turbo && deepspeed --num_gpus 2 --master_port=9901 src/train_bash.py \\\n",
    "    --deepspeed ds_config.json \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path baichuan-inc/Baichuan2-7B-Chat \\\n",
    "    --do_train True \\\n",
    "    --finetuning_type lora \\\n",
    "    --template chatglm3 \\\n",
    "    --flash_attn False \\\n",
    "    --shift_attn False \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset self_cognition \\\n",
    "    --cutoff_len 2048 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 10.0 \\\n",
    "    --max_samples 1000 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --neft_alpha 0 \\\n",
    "    --train_on_prompt False \\\n",
    "    --upcast_layernorm False \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target query_key_value \\\n",
    "    --resume_lora_training True \\\n",
    "    --output_dir saves/chatglm3/lora/law \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sr7sVbFoElmt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"导出模型\n",
    "\"\"\"\n",
    "%cd /root/autodl-tmp/LLaMA-Factory\n",
    "!mkdir out_model\n",
    "!python src/export_model.py \\\n",
    "    --model_name_or_path  ../chatglm3-6b \\\n",
    "    --template chatglm3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --checkpoint_dir saves/chatglm3/lora/law \\\n",
    "    --export_dir out_model/law_chatglm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkfPWnYOERj1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"查看模型效果\n",
    "\"\"\"\n",
    "\n",
    "%cd /root/autodl-tmp/LLaMA-Factory\n",
    "\n",
    "import os\n",
    "from threading import Thread\n",
    "\n",
    "import torch\n",
    "from transformers.generation.streamers import TextIteratorStreamer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, AutoModel\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model_name = \"/root/autodl-tmp/LLaMA-Factory/out_model/law_chatglm3/\"\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).cuda()\n",
    "model = model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "def stream(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', return_attention_mask=True)\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        decode_kwargs={\"skip_special_tokens\": True})\n",
    "    Thread(\n",
    "        target=model.generate, kwargs=dict(\n",
    "            inputs, streamer=streamer,\n",
    "            max_new_tokens=1024)\n",
    "    ).start()\n",
    "\n",
    "    start = False\n",
    "    for text in streamer:\n",
    "        if not start:\n",
    "            if prompt in text:\n",
    "                start = True\n",
    "            continue\n",
    "\n",
    "        if not text:\n",
    "            continue\n",
    "        if tokenizer.eos_token in text:\n",
    "            break\n",
    "\n",
    "        yield text\n",
    "\n",
    "    yield \"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    welcome_prompt = \"欢迎使用 法律AI 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\"\n",
    "    print(welcome_prompt)\n",
    "    while True:\n",
    "        query = input(\"\\n用户：\")\n",
    "        if query.strip() == \"stop\":\n",
    "            break\n",
    "        if query.strip() == \"clear\":\n",
    "            os.system(\"clr\")\n",
    "            print(welcome_prompt)\n",
    "            continue\n",
    "        print(\"\\n法律AI：\", end=\"\")\n",
    "        for text in stream(query):\n",
    "            print(text, end=\"\", flush=True)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
