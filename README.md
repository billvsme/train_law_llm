âœï¸LLMå¾®è°ƒä¸Šæ‰‹é¡¹ç›®
==============

ä¸€æ­¥ä¸€æ­¥ä½¿ç”¨Colabè®­ç»ƒæ³•å¾‹LLMï¼ŒåŸºäº[microsoft/phi-1_5](https://huggingface.co/microsoft/phi-1_5) ,[ChatGLM3-6B](https://huggingface.co/THUDM/chatglm3-6b)ã€‚é€šè¿‡æœ¬é¡¹ç›®ä½ å¯ä»¥0æˆæœ¬æ‰‹åŠ¨äº†è§£å¾®è°ƒLLMã€‚å¦‚æœæƒ³è¦äº†è§£LLMå¾®è°ƒå…·ä½“ä»£ç å®ç°ï¼Œ**å¯ä»¥å‚è€ƒ [my_finetune](https://github.com/billvsme/my_finetune) é¡¹ç›®**ğŸ¤“ã€‚

| name | Colab | Datasets
| --- | --- | --- 
è‡ªæˆ‘è®¤çŸ¥ lora-SFT å¾®è°ƒ | [![Colab](https://img.shields.io/badge/âœï¸-Colab-important)](https://colab.research.google.com/github/billvsme/train_law_llm/blob/master/colab/train_self_cognition.ipynb) | [self_cognition.json](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/self_cognition.json)  
æ³•å¾‹é—®ç­” lora-SFT å¾®è°ƒ | [![Colab](https://img.shields.io/badge/âœï¸-Colab-important)](https://colab.research.google.com/github/billvsme/train_law_llm/blob/master/colab/train_law.ipynb) | [DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)  
æ³•å¾‹é—®ç­” å…¨å‚æ•°-SFT å¾®è°ƒ* | [![Colab](https://img.shields.io/badge/âœï¸-Colab-important)](https://colab.research.google.com/github/billvsme/train_law_llm/blob/master/colab/train_law_full.ipynb) | [DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)  
ChatGLM3-6B è‡ªæˆ‘è®¤çŸ¥ lora-SFT å¾®è°ƒ* | [![Colab](https://img.shields.io/badge/âœï¸-Colab-important)](https://colab.research.google.com/github/billvsme/train_law_llm/blob/master/colab/train_law_chatglm3.ipynb) | [self_cognition.json](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/self_cognition.json)  



*å¦‚æœæ˜¯Colab Proä¼šå‘˜ç”¨æˆ·ï¼Œå¯ä»¥å°è¯•å…¨å‚æ•°-SFTå¾®è°ƒï¼Œä½¿ç”¨é«˜RAM+T4ï¼Œ1000æ¡æ•°æ®å¤§æ¦‚éœ€è¦20+å°æ—¶  
*å¦‚æœæ˜¯Colab Proä¼šå‘˜ç”¨æˆ·ï¼ŒChatGLM3-6B è‡ªæˆ‘è®¤çŸ¥lora-SFT å¾®è°ƒï¼Œä½¿ç”¨é«˜RAM+T4ï¼Œåªéœ€è¦å‡ åˆ†é’Ÿï¼Œæ•ˆæœæ¯”è¾ƒå¥½


## ç›®æ ‡
ä½¿ç”¨colabå…è´¹çš„T4æ˜¾å¡ï¼Œå®Œæˆæ³•å¾‹é—®ç­” æŒ‡ä»¤ç›‘ç£å¾®è°ƒ(SFT) [microsoft/phi-1_5](https://huggingface.co/microsoft/phi-1_5) æ¨¡å‹  


## è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒ
è‡ªæˆ‘è®¤çŸ¥æ•°æ®æ¥æºï¼š[self_cognition.json](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/self_cognition.json)  

80æ¡æ•°æ®ï¼Œä½¿ç”¨T4 loraå¾®è°ƒphi-1_5ï¼Œå‡ åˆ†é’Ÿå°±å¯ä»¥å¾®è°ƒå®Œæ¯•  

**å¾®è°ƒå‚æ•°**ï¼Œå…·ä½“æ­¥éª¤è¯¦è§colab
```
python src/train_bash.py \
    --stage sft \
    --model_name_or_path microsoft/phi-1_5 \
    --do_train True\
    --finetuning_type lora \
    --template vanilla \
    --flash_attn False \
    --shift_attn False \
    --dataset_dir data \
    --dataset self_cognition \
    --cutoff_len 1024 \
    --learning_rate 2e-04 \
    --num_train_epochs 20.0 \
    --max_samples 1000 \
    --per_device_train_batch_size 6 \
    --per_device_eval_batch_size 6 \
    --gradient_accumulation_steps 1 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --neft_alpha 0 \
    --train_on_prompt False \
    --upcast_layernorm False \
    --lora_rank 8 \
    --lora_dropout 0.1 \
    --lora_target Wqkv \
    --resume_lora_training True \
    --output_dir saves/Phi1.5-1.3B/lora/my \
    --fp16 True \
    --plot_loss True
```

**æ•ˆæœ**

<a href="https://sm.ms/image/gXNOy3lHdmeqv5j" target="_blank"><img src="https://s2.loli.net/2023/11/07/gXNOy3lHdmeqv5j.png" width="60%"></a>

## æ³•å¾‹é—®ç­”å¾®è°ƒ
æ³•å¾‹é—®ç­”æ•°æ®æ¥æºï¼š[DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)  
ä¸ºäº†å‡çœæ˜¾å­˜ï¼Œä½¿ç”¨deepspeed stage2ï¼Œcutoff_lenå¯ä»¥æœ€å¤šåˆ°1792ï¼Œå†å¤šå°±è¦çˆ†æ˜¾å­˜äº†  

**deepspeedé…ç½®**
```
{
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto",
  "gradient_accumulation_steps": "auto",
  "gradient_clipping": "auto",
  "zero_allow_untested_optimizer": true,
  "fp16": {
    "enabled": "auto",
    "loss_scale": 0,
    "initial_scale_power": 16,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "overlap_comm": false,
    "contiguous_gradients": true
  }
}
```

**å¾®è°ƒå‚æ•°**  

1000æ¡æ•°æ®ï¼ŒT4å¤§æ¦‚éœ€è¦60åˆ†é’Ÿ  
```
deepspeed --num_gpus 1 --master_port=9901 src/train_bash.py \
    --deepspeed ds_config.json \
    --stage sft \
    --model_name_or_path microsoft/phi-1_5 \
    --do_train True \
    --finetuning_type lora \
    --template vanilla \
    --flash_attn False \
    --shift_attn False \
    --dataset_dir data \
    --dataset self_cognition,law_sft_triplet \
    --cutoff_len 1792 \
    --learning_rate 2e-04 \
    --num_train_epochs 5.0 \
    --max_samples 1000 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 1000 \
    --warmup_steps 0 \
    --neft_alpha 0 \
    --train_on_prompt False \
    --upcast_layernorm False \
    --lora_rank 8 \
    --lora_dropout 0.1 \
    --lora_target Wqkv \
    --resume_lora_training True \
    --output_dir saves/Phi1.5-1.3B/lora/law \
    --fp16 True \
    --plot_loss True
```

## å…¨å‚å¾®è°ƒ

å¯ä»¥é€šè¿‡ï¼Œestimate_zero3_model_states_mem_needs_all_liveæŸ¥çœ‹deepspeedå„ä¸ªZeRO stage æ‰€éœ€è¦çš„å†…å­˜ã€‚  

```
from transformers import AutoModel, AutoModelForCausalLM
from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live

model_name = "microsoft/phi-1_5"
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)
estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)
```

å¦‚å›¾æ‰€é€‚ offload_optimizer -> cpu å[microsoft/phi-1_5](https://huggingface.co/microsoft/phi-1_5) éœ€è¦32Gå†…å­˜ï¼Œcolabé«˜å†…å­˜æœ‰52Gå¯ä»¥æ»¡è¶³éœ€æ±‚ã€‚  


<a href="https://sm.ms/image/EvTL7FgRzUj69rf" target="_blank"><img src="https://s2.loli.net/2023/11/07/EvTL7FgRzUj69rf.png" width="60%"></a>

**deepspeedé…ç½®**
```
{
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto",
  "gradient_accumulation_steps": "auto",
  "gradient_clipping": "auto",
  "zero_allow_untested_optimizer": true,
  "fp16": {
    "enabled": "auto",
    "loss_scale": 0,
    "initial_scale_power": 16,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "overlap_comm": false,
    "contiguous_gradients": true
  }
}
```
```
deepspeed --num_gpus 1 --master_port=9901 src/train_bash.py \
    --deepspeed ds_config.json \
    --stage sft \
    --model_name_or_path microsoft/phi-1_5 \
    --do_train True \
    --finetuning_type full \
    --template vanilla \
    --flash_attn False \
    --shift_attn False \
    --dataset_dir data \
    --dataset self_cognition,law_sft_triplet \
    --cutoff_len 1024 \
    --learning_rate 2e-04 \
    --num_train_epochs 10.0 \
    --max_samples 1000 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 1000 \
    --warmup_steps 0 \
    --neft_alpha 0 \
    --train_on_prompt False \
    --upcast_layernorm False \
    --lora_rank 8 \
    --lora_dropout 0.1 \
    --lora_target Wqkv \
    --resume_lora_training True \
    --output_dir saves/Phi1.5-1.3B/lora/law_full \
    --fp16 True \
    --plot_loss True
```

ä¹Ÿå¯ä»¥è€ƒè™‘ä½¿ç”¨ [kaggle](https://www.kaggle.com/)ï¼Œå¯ä»¥æ¯å‘¨ä½¿ç”¨30ä¸ªå°æ—¶ï¼Œå¯ä»¥é€‰æ‹©2å¼ T4ï¼Œä½¿ç”¨ZeRO stage 3 å…¨å‚å¾®è°ƒ  

**deepspeedé…ç½®**  
```
{
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto",
  "gradient_accumulation_steps": "auto",
  "gradient_clipping": "auto",
  "zero_allow_untested_optimizer": true,
  "fp16": {
    "enabled": "auto",
    "loss_scale": 0,
    "initial_scale_power": 16,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  "zero_optimization": {
    "stage": 3,
    "overlap_comm": false,
    "contiguous_gradients": true,
    "sub_group_size": 5e7,
    "reduce_bucket_size": "auto",
    "stage3_prefetch_bucket_size": "auto",
    "stage3_param_persistence_threshold": "auto",
    "stage3_max_live_parameters": 5e7,
    "stage3_max_reuse_distance": 5e7,
    "stage3_gather_16bit_weights_on_model_save": true
  }
}
```



