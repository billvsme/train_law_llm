{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdJmJPb07U47"
      },
      "outputs": [],
      "source": [
        "\"\"\"下载LLaMA-Factory\n",
        "\"\"\"\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h43G1zhf7msU"
      },
      "outputs": [],
      "source": [
        "\"\"\"安装依赖\n",
        "\"\"\"\n",
        "%cd /content/LLaMA-Factory\n",
        "!pip install -r requirements.txt\n",
        "!pip install einops\n",
        "!pip install transformers==4.34.0\n",
        "!pip install deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFqfOfzQ-8Sd"
      },
      "outputs": [],
      "source": [
        "\"\"\"替换自我认知self_cognition数据集中的名称\n",
        "\"\"\"\n",
        "%cd /content/LLaMA-Factory\n",
        "!sed -i 's/<NAME>/法律AI/g' data/self_cognition.json\n",
        "!sed -i 's/<AUTHOR>/billvsme/g' data/self_cognition.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZmSHoI_8Jo6"
      },
      "outputs": [],
      "source": [
        "\"\"\"训练\n",
        "指令监督微调，lora方式，使用self_cognition数据集\n",
        "\n",
        "由于没有对话历史，template使用vanilla\n",
        "\"\"\"\n",
        "%cd /content/LLaMA-Factory\n",
        "!rm -rf saves/Phi1.5-1.3B/lora/my\n",
        "!python src/train_bash.py \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/phi-1_5 \\\n",
        "    --do_train True\\\n",
        "    --finetuning_type lora \\\n",
        "    --template vanilla \\\n",
        "    --flash_attn False \\\n",
        "    --shift_attn False \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset self_cognition \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 2e-04 \\\n",
        "    --num_train_epochs 20.0 \\\n",
        "    --max_samples 1000 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --per_device_eval_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 100 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --neft_alpha 0 \\\n",
        "    --train_on_prompt False \\\n",
        "    --upcast_layernorm False \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_dropout 0.1 \\\n",
        "    --lora_target Wqkv \\\n",
        "    --resume_lora_training True \\\n",
        "    --output_dir saves/Phi1.5-1.3B/lora/my \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr7sVbFoElmt"
      },
      "outputs": [],
      "source": [
        "\"\"\"导出模型\n",
        "\"\"\"\n",
        "%cd /content/LLaMA-Factory\n",
        "!mkdir out_model\n",
        "!python src/export_model.py \\\n",
        "    --model_name_or_path  microsoft/phi-1_5 \\\n",
        "    --template vanilla \\\n",
        "    --finetuning_type lora \\\n",
        "    --checkpoint_dir saves/Phi1.5-1.3B/lora/my \\\n",
        "    --export_dir out_model/my"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkfPWnYOERj1"
      },
      "outputs": [],
      "source": [
        "\"\"\"查看模型效果\n",
        "\"\"\"\n",
        "\n",
        "%cd /content/LLaMA-Factory\n",
        "\n",
        "import os\n",
        "from threading import Thread\n",
        "\n",
        "import torch\n",
        "from transformers.generation.streamers import TextIteratorStreamer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "\n",
        "torch.set_default_device(\"cuda\")\n",
        "\n",
        "model_name = \"/content/LLaMA-Factory/out_model/my/\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
        "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def stream(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, return_attention_mask=True)\n",
        "    streamer = TextIteratorStreamer(\n",
        "        tokenizer,\n",
        "        decode_kwargs={\"skip_special_tokens\": True})\n",
        "    Thread(\n",
        "        target=model.generate, kwargs=dict(\n",
        "            inputs, streamer=streamer,\n",
        "            max_new_tokens=128)\n",
        "    ).start()\n",
        "\n",
        "    first = True\n",
        "    for text in streamer:\n",
        "        if first and text:\n",
        "            first = False\n",
        "            continue\n",
        "\n",
        "        if not text:\n",
        "            continue\n",
        "        if tokenizer.eos_token in text:\n",
        "            break\n",
        "\n",
        "        yield text\n",
        "\n",
        "\n",
        "def main():\n",
        "    welcome_prompt = \"欢迎使用 法律AI 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\"\n",
        "    print(welcome_prompt)\n",
        "    while True:\n",
        "        query = input(\"\\n用户：\")\n",
        "        if query.strip() == \"stop\":\n",
        "            break\n",
        "        if query.strip() == \"clear\":\n",
        "            os.system(\"clr\")\n",
        "            print(welcome_prompt)\n",
        "            continue\n",
        "        print(\"\\n法律AI：\", end=\"\")\n",
        "        for text in stream(query):\n",
        "            print(text, end=\"\", flush=True)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"保存训练好的模型到Google云盘\n",
        "\"\"\"\n",
        "%cd /content\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!mkdir /content/drive/MyDrive/llm_model/\n",
        "!cp -r /content/LLaMA-Factory/out_model/my /content/drive/MyDrive/llm_model/\n"
      ],
      "metadata": {
        "id": "sRGdH5zIf6KR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}